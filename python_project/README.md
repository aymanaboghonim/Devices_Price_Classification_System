# Devices Price Classification System - Python Project## Project DescriptionThis Python project predicts the price range of devices based on their specifications. The model helps sellers classify device prices according to their characteristics.## DatasetThe dataset used for training and testing the model includes various features such as battery power, Bluetooth support, clock speed, dual SIM support, and more.## Project Structure- `data/`: Contains raw and processed data files.- `models/`: Contains saved machine learning models.- `notebooks/`: Jupyter notebooks for exploration and documentation.- `src/`: Source code for data preparation, model training, and evaluation scripts.  - `data_preparation_and_eda.py`: Script for data preparation and exploratory data analysis.  - `model_development_and_evaluation.py`: Script for model training, tuning, and evaluation.- `app.py`: Main script to run the prediction API.- `requirements.txt`: Python dependencies.## How to Run### Setup1. **Clone the repository**:    ```bash    git clone https://github.com/aymanaboghonim/Devices_Price_Classification_System.git    cd Devices_Price_Classification_System/python_project    ```2. **Set up a virtual environment**:    ```bash    python3 -m venv venv    source venv/bin/activate  # On Windows use `venv\Scripts\activate`    ```3. **Install dependencies**:    ```bash    pip install -r requirements.txt    ```### Data Preparation and EDA1. **Run the data preparation and EDA script**:    ```bash    python src/data_preparation_and_eda.py    ```### Model Training and Evaluation1. **Run the model training and evaluation script**:    ```bash    python src/model_development_and_evaluation.py    ```### Running the API1. **Run the API**:    ```bash    python app.py    ```## API Endpoints- `POST /predict`: Predict the price range of a device based on its specifications.## Testing1. **Test Predictions for 10 Devices**:    - Modify the `model_development_and_evaluation.py` script to include predictions for 10 devices from the test dataset.    - Example code to add at the end of `model_development_and_evaluation.py`:    ```python    # Load the test data    test_data = pd.read_csv("../data/test.csv")    # Select 10 devices for prediction    test_samples = test_data.sample(10)    # Prepare the data as per the model requirements    test_samples_prepared = scaler.transform(test_samples.drop(columns=['id']))    # Predict the price range for the selected devices    predictions = best_logistic_model.predict(test_samples_prepared)    # Print the predictions    print("Predictions for 10 test devices:")    for i, prediction in enumerate(predictions):        print(f"Device {i+1}: Predicted price range = {prediction}")    ```## Data Storage1. **Choose a Database**:    - You can use SQLite for simplicity. Below are the steps to set up and store the predictions.2. **Setting up SQLite**:    - Install SQLite3:      ```bash      pip install sqlite3      ```    - Add the following code to `model_development_and_evaluation.py` to store predictions:    ```python    import sqlite3    # Connect to SQLite database (or create it if it doesn't exist)    conn = sqlite3.connect('../data/devices_predictions.db')    cursor = conn.cursor()    # Create a table for storing predictions    cursor.execute('''    CREATE TABLE IF NOT EXISTS predictions (        id INTEGER PRIMARY KEY,        device_id INTEGER,        prediction INTEGER    )    ''')    # Insert predictions into the database    for i, prediction in enumerate(predictions):        cursor.execute('''        INSERT INTO predictions (device_id, prediction)        VALUES (?, ?)        ''', (int(test_samples.iloc[i]['id']), int(prediction)))    # Commit the transaction and close the connection    conn.commit()    conn.close()    ```## Model Training and Evaluation- **Data Preparation**: Cleaning and scaling data.- **EDA**: Insights and visualizations.- **Model Training**: Training using various algorithms.- **Evaluation**: Metrics like confusion matrix to evaluate model performance.- **Model Optimization**: Hyperparameter tuning for logistic regression using GridSearchCV.## Additional Information- **Comments and Documentation**: Each step of the data preparation, model training, and evaluation processes is documented with comments explaining the rationale behind each decision.## Authors- [Ayman Abo Ghonim](https://github.com/aymanaboghonim)